{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "shared-mineral",
   "metadata": {},
   "source": [
    "# Build Your First Machine Learning Model!\n",
    "\n",
    "Welome to the **Machine Learning** programming assignment of the **Artificial Intelligence** course.\n",
    "\n",
    "In this assignment, a Jupyter Notebook is prepared for examining almost all critical factors in designing a Machine Leaning model for a supervised learning task.\n",
    "\n",
    "You will complete this notebook to build a ML model, and apply it to a binary classification problem. Additionally you will get familiar with varoius factors which may enhance the performance of a ML model.\n",
    "\n",
    "In this notebook, we will take advantage of four principal python libraries for data science and machine learning tasks.\n",
    "\n",
    " - [Numpy](https://numpy.org): The fundamental package for **scientific computing** with Python.\n",
    " - [Pandas](https://pandas.pydata.org): An open source **data analysis and manipulation tool**, built on top of the Python programming language.\n",
    " - [Matplotlib](https://matplotlib.org) : A comprehensive library for **creating static, animated, and interactive visualizations** in Python.\n",
    " - [Scikit-Learn](https://scikit-learn.org): Simple and efficient tools for predictive data analysis and building Machine Learning models.\n",
    " \n",
    "**After this assignment you will be able to:**\n",
    "1. Run Exploratory Data Analysis (EDA) and know how to prepare data for a predictive model.\n",
    "2. Build and apply a Machine Learning model for a supervised learning problem using known frameworks.\n",
    " \n",
    " \n",
    " **Before you start:** Please read the ***Submission*** section at the bottom of the notebook carefully.\n",
    " \n",
    " Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-monkey",
   "metadata": {},
   "source": [
    "\n",
    "# 0. User UI Preference - Introduction\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/UI.png\" width=1000>\n",
    "</p>\n",
    "\n",
    "The **User Interface (UI)** is the graphical layout of an application. It consists of the buttons users click on, the text they read, the images, sliders, text entry fields, and all the rest of the items the user interacts with. This includes screen layout, transitions, interface animations and every single micro-interaction. Any sort of visual element, interaction, or animation must all be designed.\n",
    "\n",
    "An ecommerce company has changed the UI of it's website recently but they do not know how it is possible to evaluate their new UI. Do customers like it? Do they prefer the new UI or the old one?\n",
    "\n",
    "They have also collected feedbacks from their customers based on some pre-organized surveys. You as Machine Learning engineers are aksed to **build a Machine Learning Model so to identify the User UI preference on the basis of their UI engagement information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-mayor",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-township",
   "metadata": {},
   "source": [
    "There are two files that you need to consider. *(i)* `train.csv` for training, and *(ii)* `test.csv` for testing purposes. They consist 14 columns with the following description:\n",
    "\n",
    "\n",
    "| Column name | Description |\n",
    "| ------------- | ------------- |\n",
    "| **CustomerID** | Represents a unique identification of a user |\n",
    "| **Age** | Represents the age of the user |\n",
    "| **City** | Represents the city in which the user lives |\n",
    "| **State** | Represents the state in which the user lives |\n",
    "| **No_of_orders_placed** | Represents the total number of orders placed by a customer |\n",
    "| **Last order placed_date** | Represents the last date when the customer placed the order |\n",
    "| **is premium_member** | Represents whether a customer is a premium member or not. ( 0 or 1) |\n",
    "| **Women's Clothing** | Represents user's engagement score in Women's_Clothing section ( 0 to 10 ) |\n",
    "| **Men's Clothing** | Represents user's engagement score in Men's Clothing section( 0 to 10 ) |\n",
    "| **Kid's Clothing** | Represents user's engagement score in Kid's Clothing section ( 0 to 10 ) |\n",
    "| **Home &_Living** | Represents user's engagement score in Home_&_Living section ( 0 to 10 ) |\n",
    "| **Beauty** | Represents user's engagement score in Beauty products section ( 0 to 10 ) |\n",
    "| **Electronics** | Represents user's engagement score in Electronics products section( 0 to 10 ) |\n",
    "| **Preferred_Theme** | Represents the preferred theme ( Old_UI or New_UI) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-ivory",
   "metadata": {},
   "source": [
    "## Packages\n",
    "Let's first import all the packages that you will need during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "identified-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-parliament",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing\n",
    "\n",
    "\n",
    "What's Data preprocessing? **Data preprocessing** is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.\n",
    "\n",
    "Some of the main preprocessing steps are:\n",
    "\n",
    "  - **Handling missing values**: impute missing (NaN) values\n",
    "  - **Standardization**: normalize our data for a more quick convergence of the model\n",
    "  - **Handling Categorical Variables**: one-hot encoding is an appropirate solution\n",
    "  - ...\n",
    "  \n",
    "In this assignment, you will apply some of these methods step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "vanilla-abraham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10605\n",
      "Number of test samples: 4545\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>No_of_orders_placed</th>\n",
       "      <th>Sign_up_date</th>\n",
       "      <th>Last_order_placed_date</th>\n",
       "      <th>is_premium_member</th>\n",
       "      <th>Women’s_Clothing</th>\n",
       "      <th>Men’s_Clothing</th>\n",
       "      <th>Kid’s_Clothing</th>\n",
       "      <th>Home_&amp;_Living</th>\n",
       "      <th>Beauty</th>\n",
       "      <th>Electronics</th>\n",
       "      <th>Preferred_Theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CusID_00685</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Not_Specified</td>\n",
       "      <td>Bercelona</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>2020-09-19</td>\n",
       "      <td>1</td>\n",
       "      <td>3.445937</td>\n",
       "      <td>2.620136</td>\n",
       "      <td>5.457500</td>\n",
       "      <td>6.141905</td>\n",
       "      <td>8.106490</td>\n",
       "      <td>4.389933</td>\n",
       "      <td>Old_UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CusID_06121</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>0</td>\n",
       "      <td>1.320304</td>\n",
       "      <td>9.025863</td>\n",
       "      <td>6.378695</td>\n",
       "      <td>2.825636</td>\n",
       "      <td>0.977430</td>\n",
       "      <td>7.789076</td>\n",
       "      <td>New_UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CusID_09847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not_Specified</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>2021-10-13</td>\n",
       "      <td>1</td>\n",
       "      <td>3.793410</td>\n",
       "      <td>0.726490</td>\n",
       "      <td>3.957772</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.589856</td>\n",
       "      <td>7.426173</td>\n",
       "      <td>Old_UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CusID_01433</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Not_Specified</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2016-02-27</td>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>1</td>\n",
       "      <td>2.362948</td>\n",
       "      <td>2.701855</td>\n",
       "      <td>3.522246</td>\n",
       "      <td>3.121818</td>\n",
       "      <td>2.259882</td>\n",
       "      <td>5.970419</td>\n",
       "      <td>Old_UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CusID_02167</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>0</td>\n",
       "      <td>7.568971</td>\n",
       "      <td>2.161103</td>\n",
       "      <td>7.535511</td>\n",
       "      <td>0.865851</td>\n",
       "      <td>4.638520</td>\n",
       "      <td>9.061972</td>\n",
       "      <td>Old_UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CusID_02674</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>2021-02-21</td>\n",
       "      <td>0</td>\n",
       "      <td>8.790859</td>\n",
       "      <td>4.219936</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.395232</td>\n",
       "      <td>9.539830</td>\n",
       "      <td>1.066810</td>\n",
       "      <td>Old_UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CusID_05594</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Kuala Lampur</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2017-09-29</td>\n",
       "      <td>2020-07-24</td>\n",
       "      <td>0</td>\n",
       "      <td>7.496507</td>\n",
       "      <td>6.065087</td>\n",
       "      <td>6.757594</td>\n",
       "      <td>8.390672</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.440012</td>\n",
       "      <td>New_UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CusID_09297</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Munich</td>\n",
       "      <td>Catalonia</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>0</td>\n",
       "      <td>7.447626</td>\n",
       "      <td>0.724823</td>\n",
       "      <td>8.645873</td>\n",
       "      <td>7.822508</td>\n",
       "      <td>7.069303</td>\n",
       "      <td>2.882355</td>\n",
       "      <td>New_UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CusID_03771</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Vienna</td>\n",
       "      <td>Vienna</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-04-02</td>\n",
       "      <td>2021-02-15</td>\n",
       "      <td>0</td>\n",
       "      <td>7.573536</td>\n",
       "      <td>4.267514</td>\n",
       "      <td>4.359906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.521518</td>\n",
       "      <td>0.482545</td>\n",
       "      <td>New_UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CusID_00667</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>London</td>\n",
       "      <td>England</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2017-08-11</td>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>1</td>\n",
       "      <td>5.061261</td>\n",
       "      <td>3.822283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.377240</td>\n",
       "      <td>9.669255</td>\n",
       "      <td>2.410884</td>\n",
       "      <td>Old_UI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CustomerID   Age         Gender          City             State  \\\n",
       "0  CusID_00685  19.0  Not_Specified     Bercelona         Singapore   \n",
       "1  CusID_06121  31.0           Male        Sydney   New South Wales   \n",
       "2  CusID_09847   NaN  Not_Specified       Toronto           Ontario   \n",
       "3  CusID_01433  29.0  Not_Specified       Toronto           Ontario   \n",
       "4  CusID_02167  38.0         Female           NaN  British Columbia   \n",
       "5  CusID_02674  30.0         Female       Kolkata       West Bengal   \n",
       "6  CusID_05594  27.0         Female  Kuala Lampur         Singapore   \n",
       "7  CusID_09297  27.0         Female        Munich         Catalonia   \n",
       "8  CusID_03771  18.0         Female        Vienna            Vienna   \n",
       "9  CusID_00667  19.0         Female        London           England   \n",
       "\n",
       "   No_of_orders_placed Sign_up_date Last_order_placed_date  is_premium_member  \\\n",
       "0                 14.0   2017-01-17             2020-09-19                  1   \n",
       "1                 10.0   2016-01-22             2021-12-09                  0   \n",
       "2                 12.0   2019-08-07             2021-10-13                  1   \n",
       "3                 15.0   2016-02-27             2020-10-22                  1   \n",
       "4                  3.0   2019-07-04             2020-03-17                  0   \n",
       "5                 10.0   2017-07-29             2021-02-21                  0   \n",
       "6                  7.0   2017-09-29             2020-07-24                  0   \n",
       "7                  7.0   2018-04-18             2020-03-15                  0   \n",
       "8                  4.0   2018-04-02             2021-02-15                  0   \n",
       "9                 13.0   2017-08-11             2020-01-09                  1   \n",
       "\n",
       "   Women’s_Clothing  Men’s_Clothing  Kid’s_Clothing  Home_&_Living    Beauty  \\\n",
       "0          3.445937        2.620136        5.457500       6.141905  8.106490   \n",
       "1          1.320304        9.025863        6.378695       2.825636  0.977430   \n",
       "2          3.793410        0.726490        3.957772       3.000000  2.589856   \n",
       "3          2.362948        2.701855        3.522246       3.121818  2.259882   \n",
       "4          7.568971        2.161103        7.535511       0.865851  4.638520   \n",
       "5          8.790859        4.219936        3.000000       6.395232  9.539830   \n",
       "6          7.496507        6.065087        6.757594       8.390672  2.000000   \n",
       "7          7.447626        0.724823        8.645873       7.822508  7.069303   \n",
       "8          7.573536        4.267514        4.359906            NaN  6.521518   \n",
       "9          5.061261        3.822283             NaN       7.377240  9.669255   \n",
       "\n",
       "   Electronics Preferred_Theme  \n",
       "0     4.389933          Old_UI  \n",
       "1     7.789076          New_UI  \n",
       "2     7.426173          Old_UI  \n",
       "3     5.970419          Old_UI  \n",
       "4     9.061972          Old_UI  \n",
       "5     1.066810          Old_UI  \n",
       "6     3.440012          New_UI  \n",
       "7     2.882355          New_UI  \n",
       "8     0.482545          New_UI  \n",
       "9     2.410884          Old_UI  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset into a dataframe\n",
    "train_df = pd.read_csv('data/train.csv', index_col=0,na_values='?')\n",
    "test_df = pd.read_csv('data/test.csv', index_col=0,na_values='?')\n",
    "\n",
    "print(f'Number of training samples: {len(train_df)}')\n",
    "print(f'Number of test samples: {len(test_df)}')\n",
    "train_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-marketing",
   "metadata": {},
   "source": [
    "## 1.1 Missing values (10 points):\n",
    "\n",
    "In any real-world dataset, there are always few null values. It doesn’t really matter whether it is a regression, classification or any other kind of problem, no model can handle these NULL or NaN values on its own so we need to intervene.\n",
    "\n",
    "```\n",
    "- In python NULL is reprsented with NaN. So don’t get confused between these two,they can be used interchangably.\n",
    "```\n",
    "\n",
    "First of all, we need to check whether we have null values in our dataset or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "expensive-courtesy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "CustomerID                  0\n",
      "Age                       729\n",
      "Gender                      0\n",
      "City                      311\n",
      "State                       0\n",
      "No_of_orders_placed       535\n",
      "Sign_up_date              102\n",
      "Last_order_placed_date      0\n",
      "is_premium_member           0\n",
      "Women’s_Clothing            0\n",
      "Men’s_Clothing              0\n",
      "Kid’s_Clothing            648\n",
      "Home_&_Living             595\n",
      "Beauty                      0\n",
      "Electronics                 0\n",
      "Preferred_Theme             0\n",
      "dtype: int64\n",
      "============================\n",
      "Test set:\n",
      "CustomerID                  0\n",
      "Age                       274\n",
      "Gender                      0\n",
      "City                      135\n",
      "State                       0\n",
      "No_of_orders_placed       238\n",
      "Sign_up_date               52\n",
      "Last_order_placed_date      0\n",
      "is_premium_member           0\n",
      "Women’s_Clothing            0\n",
      "Men’s_Clothing              0\n",
      "Kid’s_Clothing            287\n",
      "Home_&_Living             253\n",
      "Beauty                      0\n",
      "Electronics                 0\n",
      "Preferred_Theme             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(train_df.isna().sum())\n",
    "print(\"============================\")\n",
    "print(\"Test set:\")\n",
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-tenant",
   "metadata": {},
   "source": [
    "There are various ways for us to handle this problem. The easiest way to solve this problem is by dropping the rows or columns that contain null values.\n",
    "\n",
    "Here is a link for you too choose your method and then apply it on both train and test set:\n",
    "https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aboriginal-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "CustomerID                0\n",
      "Age                       0\n",
      "Gender                    0\n",
      "City                      0\n",
      "State                     0\n",
      "No_of_orders_placed       0\n",
      "Sign_up_date              0\n",
      "Last_order_placed_date    0\n",
      "is_premium_member         0\n",
      "Women’s_Clothing          0\n",
      "Men’s_Clothing            0\n",
      "Kid’s_Clothing            0\n",
      "Home_&_Living             0\n",
      "Beauty                    0\n",
      "Electronics               0\n",
      "Preferred_Theme           0\n",
      "dtype: int64\n",
      "============================\n",
      "Test set:\n",
      "CustomerID                0\n",
      "Age                       0\n",
      "Gender                    0\n",
      "City                      0\n",
      "State                     0\n",
      "No_of_orders_placed       0\n",
      "Sign_up_date              0\n",
      "Last_order_placed_date    0\n",
      "is_premium_member         0\n",
      "Women’s_Clothing          0\n",
      "Men’s_Clothing            0\n",
      "Kid’s_Clothing            0\n",
      "Home_&_Living             0\n",
      "Beauty                    0\n",
      "Electronics               0\n",
      "Preferred_Theme           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# choose an appropriate way to impute the missing values\n",
    "# Please  mention the method you have selected by writing in a markdown cell.\n",
    "\n",
    "################ YOUR CODE STARTS HERE ################\n",
    "train_df = train_df.dropna(how = 'all')\n",
    "train_df = train_df.fillna(method ='bfill')\n",
    "\n",
    "test_df = test_df.dropna(how = 'all')\n",
    "test_df = test_df.fillna(method ='bfill')\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(train_df.isna().sum())\n",
    "print(\"============================\")\n",
    "print(\"Test set:\")\n",
    "print(test_df.isna().sum())\n",
    "################ YOUR CODE ENDS HERE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-knowing",
   "metadata": {},
   "source": [
    "## 1.2 Standardization (10 points):\n",
    "\n",
    "It is another integral preprocessing step. In Standardization, we transform our values such that the mean of the values is 0 and the standard deviation is 1.\n",
    "\n",
    "**Hint:** Use `sklearn.preprocessing.StandardScaler` to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "personal-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize your train/test data.\n",
    "# Important note: You must use the mean and standard deviation of train data for test set! You know why :)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "################ YOUR CODE STARTS HERE ################\n",
    "numeric_cols = ['No_of_orders_placed','Women’s_Clothing','Men’s_Clothing','Kid’s_Clothing','Home_&_Living','Beauty','Electronics']\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[numeric_cols].values)\n",
    "train_df_scaled_numeric = scaler.transform(train_df[numeric_cols])\n",
    "test_df_scaled_numeric = scaler.transform(test_df[numeric_cols])\n",
    "train_df_scaled_numeric = pd.DataFrame(train_df_scaled_numeric,columns=numeric_cols)\n",
    "test_df_scaled_numeric = pd.DataFrame(test_df_scaled_numeric,columns=numeric_cols)\n",
    "################ YOUR CODE ENDS HERE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-conversation",
   "metadata": {},
   "source": [
    "## 1.3 Handling categorical Variables (10 points):\n",
    "\n",
    "Handling categorical variables is another integral aspect of Machine Learning. Categorical variables are basically the variables that are discrete and not continuous. One of the methods to do this is **One-hot encoding**:\n",
    "\n",
    "**-One-hot encoding:**\n",
    "\n",
    "For categorical variables where no such ordinal relationship exists, the integer encoding is not enough.\n",
    "\n",
    "In fact, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n",
    "\n",
    "In this case, a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n",
    "\n",
    "In the “color” variable example, there are 3 categories and therefore 3 binary variables are needed. A “1” value is placed in the binary variable for the color and “0” values for the other colors.\n",
    "\n",
    "```\n",
    "red,\tgreen,\tblue\n",
    "1,\t\t0,\t\t0\n",
    "0,\t\t1,\t\t0\n",
    "0,\t\t0,\t\t1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "protected-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you have to convert the categorical features in the \n",
    "# dataset to one-hot encoded representation.\n",
    "# Hint: Use pd.DataFrame.get_dummies()\n",
    "\n",
    "# before that, we drop 'City' column from both training and test set, we wont use it.\n",
    "train_df.drop('City', axis=1, inplace=True)\n",
    "test_df.drop('City', axis=1, inplace=True)\n",
    "\n",
    "cols_to_be_encoded = ['Gender', 'State']\n",
    "\n",
    "################ YOUR CODE STARTS HERE ################\n",
    "\n",
    "train_df_encoded = pd.get_dummies(train_df[cols_to_be_encoded], prefix=['Gender', 'State'])\n",
    "test_df_encoded = pd.get_dummies(test_df[cols_to_be_encoded], prefix=['Gender', 'State'])\n",
    "\n",
    "################ YOUR CODE ENDS HERE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-nightmare",
   "metadata": {},
   "source": [
    "The preprocess steps you have done till now,were the most essential methods to preprocess a raw data. Otherwise, you are free to do more on this phase.\n",
    "\n",
    "For instance, **Feature Engineering** is a great example of preprocessing steps. It includes creating new features by mixing existing variables in order to help the ML model classify the sampels more accuratly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-connectivity",
   "metadata": {},
   "source": [
    "# 2. Designing ML models\n",
    "\n",
    "In this section, we are going to build, compile and train our ML models. There are a huge number of various models which you can use and fortunatly, almost all of them has been implemented in [Scikit-Learn](https://scikit-learn.org).\n",
    "\n",
    "However in this assignment, **you have to choose one of the following ML algorithms**, and implement it from scratch **without using existing frameworks**.\n",
    "\n",
    "   - **KNN**\n",
    "   - **Naive Bayes**\n",
    "   \n",
    "There are a plenty of resourcse and tutorials for these two algorithm on the internet. Just search them and read about them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-response",
   "metadata": {},
   "source": [
    "## 2.1 KNN or Naive Bayes from scratch (25 points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "taken-membership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6360207449316361\n"
     ]
    }
   ],
   "source": [
    "# Implement one of above-mentioned algorithms from scratch.\n",
    "# You are free to use Numpy and also pandas functionalities to implement them.\n",
    "# But avoid using sklearn!\n",
    "\n",
    "# Before that use train_test_split() to split your train data into train/validation\n",
    "# in order to evaluate your model based on validation data\n",
    "\n",
    "#merge all columns\n",
    "train_df = pd.concat([train_df_encoded, train_df_scaled_numeric,train_df['Preferred_Theme']], axis=1)\n",
    "test_df = pd.concat([test_df_encoded, test_df_scaled_numeric,test_df['Preferred_Theme']], axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df.drop('Preferred_Theme', axis=1),\n",
    "                                                    train_df['Preferred_Theme'],\n",
    "                                                    test_size=0.2)\n",
    "################ YOUR CODE STARTS HERE ################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def minkowski_distance(a, b, p=1):\n",
    "    # Store the number of dimensions\n",
    "    dim = len(a)\n",
    "    # Set initial distance to 0\n",
    "    distance = 0\n",
    "    # Calculate minkowski distance using parameter p\n",
    "    for d in range(dim):\n",
    "        distance += abs(a[d] - b[d])**p\n",
    "    distance = distance**(1/p)\n",
    "    return distance\n",
    "\n",
    "def knn_predict(X_train, X_test, y_train, y_test, k, p):\n",
    "    # Counter to help with label voting\n",
    "    from collections import Counter\n",
    "    # Make predictions on the test data\n",
    "    # Need output of 1 prediction per test data point\n",
    "    y_hat_test = []\n",
    "    for test_point in X_test:\n",
    "        distances = []\n",
    "        for train_point in X_train:\n",
    "            distance = minkowski_distance(test_point, train_point, p=p)\n",
    "            distances.append(distance)  \n",
    "        # Store distances in a dataframe\n",
    "        df_dists = pd.DataFrame(data=distances, columns=['dist'], \n",
    "                                index=y_train.index)      \n",
    "        # Sort distances, and only consider the k closest points\n",
    "        df_nn = df_dists.sort_values(by=['dist'], axis=0)[:k]\n",
    "        # Create counter object to track the labels of k closest neighbors\n",
    "        counter = Counter(y_train[df_nn.index])\n",
    "        # Get most common label of all the nearest neighbors\n",
    "        prediction = counter.most_common()[0][0]\n",
    "        # Append prediction to output list\n",
    "        y_hat_test.append(prediction)  \n",
    "    return y_hat_test\n",
    "\n",
    "y_hat_test = knn_predict(X_train.values, X_val.values, y_train, y_val, k=3, p=1)\n",
    "print(accuracy_score(y_val, y_hat_test))\n",
    "################ YOUR CODE ENDS HERE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-monitor",
   "metadata": {},
   "source": [
    "## 2.2 Other common ML models (25 points):\n",
    "\n",
    "In this part you have to use predefined ML algorithms in [Scikit-Learn](https://scikit-learn.org) to implement other known models. **You must at least try 2 differnt models** to get the full score of this part.\n",
    "\n",
    "**Note 1:** But you are free to try other model as much as you can to get higher performances.\n",
    "\n",
    "**Note 2:** Writing about how those algorithms work has bonus!\n",
    "\n",
    "\n",
    "You can see a complete list of exisiting models in Scikit-learn documentation: https://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "drawn-details",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier:  0.5991199119911991\n",
      "SVC:  0.6941694169416942\n",
      "RandomForestClassifier:  0.6664466446644665\n"
     ]
    }
   ],
   "source": [
    "################ YOUR CODE STARTS HERE ################\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train.values, y_train.values)\n",
    "y_hat_test_DT = clf.predict(X_test.values)\n",
    "print('DecisionTreeClassifier: ',accuracy_score(y_test, y_hat_test_DT))\n",
    "\n",
    "#SVC\n",
    "clf = svm.SVC()\n",
    "clf = clf.fit(X_train.values, y_train.values)\n",
    "y_hat_test_SVC = clf.predict(X_test.values)\n",
    "print('SVC: ',accuracy_score(y_test, y_hat_test_SVC))\n",
    "\n",
    "#RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X_train.values, y_train.values)\n",
    "y_hat_test_RF = clf.predict(X_test.values)\n",
    "print('RandomForestClassifier: ',accuracy_score(y_test, y_hat_test_RF))\n",
    "################ YOUR CODE ENDS HERE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-profile",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation (20 points):\n",
    "\n",
    "Great! You are now able to evaluate your model and see how nice your algorithm is able to classify positive and negative samples both on train and validation data.\n",
    "\n",
    "For this, we are going to use 4 metrics. It is highly recommnded to read about them and know how benficial they are for different purposes:\n",
    "\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score\n",
    "\n",
    "Use `sklearn.metrics` to implement them and report the results on both train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "acute-compatibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for each label (DecisionTreeClassifier):  [0.92450331 0.91622807]\n",
      "recall for each label (DecisionTreeClassifier):  [0.91641138 0.92433628]\n",
      "fscore for each label (DecisionTreeClassifier):  [0.92043956 0.92026432]\n",
      "support for each label (DecisionTreeClassifier):  [2285 2260]\n",
      "---------------------------------\n",
      "precision for each label (SVC):  [0.72904762 0.69161554]\n",
      "recall for each label (SVC):  [0.67002188 0.74823009]\n",
      "fscore for each label (SVC):  [0.69828962 0.71880978]\n",
      "support for each label (SVC):  [2285 2260]\n",
      "---------------------------------\n",
      "precision for each label (RandomForestClassifier):  [0.90752417 0.94182825]\n",
      "recall for each label (RandomForestClassifier):  [0.94485777 0.90265487]\n",
      "fscore for each label (RandomForestClassifier):  [0.92581475 0.92182558]\n",
      "support for each label (RandomForestClassifier):  [2285 2260]\n"
     ]
    }
   ],
   "source": [
    "################ YOUR CODE STARTS HERE ################\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "precision,recall,fscore,support = precision_recall_fscore_support(y_test, y_hat_test_DT)\n",
    "print('precision for each label (DecisionTreeClassifier): ',precision)\n",
    "print('recall for each label (DecisionTreeClassifier): ',recall)\n",
    "print('fscore for each label (DecisionTreeClassifier): ',fscore)\n",
    "print('support for each label (DecisionTreeClassifier): ',support)\n",
    "print('---------------------------------')\n",
    "\n",
    "#SVC\n",
    "precision,recall,fscore,support = precision_recall_fscore_support(y_test, y_hat_test_SVC)\n",
    "print('precision for each label (SVC): ',precision)\n",
    "print('recall for each label (SVC): ',recall)\n",
    "print('fscore for each label (SVC): ',fscore)\n",
    "print('support for each label (SVC): ',support)\n",
    "print('---------------------------------')\n",
    "\n",
    "#RandomForestClassifier\n",
    "precision,recall,fscore,support = precision_recall_fscore_support(y_test, y_hat_test_RF)\n",
    "print('precision for each label (RandomForestClassifier): ',precision)\n",
    "print('recall for each label (RandomForestClassifier): ',recall)\n",
    "print('fscore for each label (RandomForestClassifier): ',fscore)\n",
    "print('support for each label (RandomForestClassifier): ',support)\n",
    "################ YOUR CODE ENDS HERE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-donor",
   "metadata": {},
   "source": [
    "# 3. Submission\n",
    "\n",
    "Please read the notes here carefully:\n",
    "\n",
    "1. In addition to completing the code files, please send a report including your answer to these questions as well. Do not forget to put the diagrams and visualizations needed in each part.\n",
    "\n",
    "2. The file you upload must be named as `[Student ID]-[Your name].zip.`\n",
    "3. Your notebook must be executed without any problem. If not, you will lose points for each part consequently.\n",
    "4. **Important Note:** The outputs of the code blocks must be remained in your notebook, otherwise, you definitly lose all the points of that part.\n",
    "\n",
    "\n",
    "\n",
    "In case you have any questions, contact **mohammad99hashemi@gmail.com**.\n",
    "\n",
    "\n",
    "Good luck :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
